"""
TimeLLM EEGåˆ†ç±»é›†æˆæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®é›†æˆ
é€‚é…RTX 2060 (6GBæ˜¾å­˜)
"""

import torch
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
project_path = r"D:\æ–‡ä»¶\æ–‡ä»¶\HKU\Dissertation\Time-LLM-main-editversion\Time-LLM-main"
sys.path.append(project_path)

print("=" * 70)
print("TimeLLM EEGåˆ†ç±»é›†æˆæµ‹è¯•")
print("=" * 70)
print(f"é¡¹ç›®è·¯å¾„: {project_path}")
print(f"Pythonè·¯å¾„å·²æ·»åŠ : {project_path in sys.path}")

# æ£€æŸ¥CUDA
print(f"\nCUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# æµ‹è¯•å¯¼å…¥
print("\n1. æµ‹è¯•æ¨¡å—å¯¼å…¥...")
try:
    from models.TimeLLM import Model
    print("âœ“ æˆåŠŸå¯¼å…¥ TimeLLM.Model")
except Exception as e:
    print(f"âœ— å¯¼å…¥TimeLLMå¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å°†ä¿®æ”¹åçš„TimeLLM.pyæ”¾åœ¨modelsæ–‡ä»¶å¤¹ä¸­")
    exit(1)

try:
    from data_provider.data_factory import data_provider
    print("âœ“ æˆåŠŸå¯¼å…¥ data_provider")
except Exception as e:
    print(f"âœ— å¯¼å…¥data_providerå¤±è´¥: {e}")
    exit(1)

# åˆ›å»ºé€‚åˆRTX 2060çš„é…ç½®
class Config:
    # ä»»åŠ¡é…ç½®
    task_name = 'classification'
    
    # æ•°æ®é…ç½®
    seq_len = 256      # EEGåºåˆ—é•¿åº¦
    pred_len = 0       # åˆ†ç±»ä»»åŠ¡ä¸éœ€è¦é¢„æµ‹é•¿åº¦
    label_len = 0      # åˆ†ç±»ä»»åŠ¡ä¸éœ€è¦æ ‡ç­¾é•¿åº¦
    enc_in = 32        # DEAPçš„32ä¸ªEEGé€šé“
    num_class = 2      # äºŒåˆ†ç±»ï¼ˆæ­£é¢/è´Ÿé¢æƒ…ç»ªï¼‰
    
    # æ¨¡å‹é…ç½®ï¼ˆé’ˆå¯¹6GBæ˜¾å­˜ä¼˜åŒ–ï¼‰
    d_model = 32       # æ¨¡å‹ç»´åº¦
    n_heads = 4        # æ³¨æ„åŠ›å¤´æ•°ï¼ˆå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
    d_ff = 64          # FFNç»´åº¦ï¼ˆå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
    dropout = 0.1
    patch_len = 16     # è¡¥ä¸é•¿åº¦
    stride = 8         # æ­¥é•¿
    
    # LLMé…ç½®ï¼ˆå…³é”®ï¼šå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
    llm_model = 'GPT2'
    llm_dim = 768     # Llama-7Bçš„ç»´åº¦
    llm_layers = 2     # åªä½¿ç”¨2å±‚ï¼ˆå¤§å¹…å‡å°‘å†…å­˜ï¼ï¼‰
    
    # å…¶ä»–å¿…éœ€å‚æ•°
    features = 'M'
    e_layers = 2
    d_layers = 1
    factor = 1
    activation = 'gelu'
    embed = 'timeF'
    freq = 'h'
    
print("\n2. åˆ›å»ºæ¨¡å‹...")
print("é…ç½®å‚æ•°:")
print(f"  - LLMå±‚æ•°: {Config.llm_layers} (é’ˆå¯¹6GBæ˜¾å­˜ä¼˜åŒ–)")
print(f"  - æ¨¡å‹ç»´åº¦: {Config.d_model}")
print(f"  - é€šé“æ•°: {Config.enc_in}")
print(f"  - ç±»åˆ«æ•°: {Config.num_class}")

try:
    config = Config()
    model = Model(config)
    print("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  - æ€»å‚æ•°é‡: {total_params/1e6:.2f}M")
    print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params/1e6:.2f}M")
    
except Exception as e:
    print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

# æµ‹è¯•å‰å‘ä¼ æ’­
print("\n3. æµ‹è¯•å‰å‘ä¼ æ’­...")
batch_size = 2  # å°æ‰¹æ¬¡æµ‹è¯•
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å°†æ¨¡å‹ç§»åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
if torch.cuda.is_available():
    try:
        model = model.to(device)
        print(f"âœ“ æ¨¡å‹å·²ç§»è‡³: {device}")
    except Exception as e:
        print(f"âœ— æ— æ³•å°†æ¨¡å‹ç§»è‡³GPU: {e}")
        device = torch.device('cpu')
        print("  ä½¿ç”¨CPUè¿è¡Œ")

# åˆ›å»ºæµ‹è¯•æ•°æ®
x = torch.randn(batch_size, config.seq_len, config.enc_in).to(device)
x_mark = torch.zeros(batch_size, config.seq_len, 4).to(device)

print(f"\nè¾“å…¥æ•°æ®å½¢çŠ¶:")
print(f"  - x: {x.shape}")
print(f"  - x_mark: {x_mark.shape}")

# å‰å‘ä¼ æ’­æµ‹è¯•
try:
    with torch.no_grad():
        # æ¸…ç©ºGPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        output = model(x, x_mark)
        
    print(f"\nâœ“ å‰å‘ä¼ æ’­æˆåŠŸ!")
    print(f"  - è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"  - æœŸæœ›å½¢çŠ¶: [batch_size={batch_size}, num_class={config.num_class}]")
    print(f"  - è¾“å‡ºç¤ºä¾‹: {output[0].cpu().numpy()}")
    
    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åˆç†
    if output.shape == (batch_size, config.num_class):
        print("\nâœ“ è¾“å‡ºç»´åº¦æ­£ç¡®ï¼")
    else:
        print("\nâœ— è¾“å‡ºç»´åº¦ä¸æ­£ç¡®")
        
except Exception as e:
    print(f"\nâœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    
    if "out of memory" in str(e).lower():
        print("\nå†…å­˜ä¸è¶³å»ºè®®:")
        print("  1. å‡å°‘ llm_layers (å½“å‰: {})".format(Config.llm_layers))
        print("  2. å‡å°‘ batch_size")
        print("  3. å‡å°‘ d_model æˆ– d_ff")

# æµ‹è¯•æ•°æ®åŠ è½½å™¨
print("\n" + "="*50)
print("4. æµ‹è¯•EEGæ•°æ®åŠ è½½å™¨...")

class DataConfig:
    # å¤ç”¨æ¨¡å‹é…ç½®
    task_name = 'classification'
    data = 'DEAP'
    root_path = r'D:\æ–‡ä»¶\æ–‡ä»¶\HKU\Dissertation\dataset\DEAP\data_preprocessed_python'
    seq_len = 256
    pred_len = 0
    label_len = 0
    batch_size = 4
    num_workers = 0
    embed = 'timeF'
    freq = 'h'
    features = 'M'
    
    # EEGç‰¹å®šå‚æ•°
    num_class = 2
    enc_in = 32
    classification_type = 'valence'
    overlap = 128
    normalize = True
    filter_freq = [0.5, 45]
    sampling_rate = 128
    subject_list = ['s01']  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªè¢«è¯•

try:
    data_args = DataConfig()
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    if os.path.exists(data_args.root_path):
        print(f"âœ“ æ•°æ®è·¯å¾„å­˜åœ¨: {data_args.root_path}")
        files = [f for f in os.listdir(data_args.root_path) if f.endswith('.dat')]
        print(f"  æ‰¾åˆ° {len(files)} ä¸ªæ•°æ®æ–‡ä»¶")
        if files:
            print(f"  ç¤ºä¾‹æ–‡ä»¶: {files[:3]}")
    else:
        print(f"âœ— æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_args.root_path}")
        print("  è·³è¿‡æ•°æ®åŠ è½½æµ‹è¯•")
        exit(0)
    
    # å°è¯•åŠ è½½æ•°æ®
    print("\nå°è¯•åŠ è½½DEAPæ•°æ®...")
    train_data, train_loader = data_provider(data_args, 'train')
    print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ!")
    print(f"  - è®­ç»ƒé›†å¤§å°: {len(train_data)}")
    print(f"  - æ‰¹æ¬¡æ•°: {len(train_loader)}")
    
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡
    for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:
        print(f"\næ‰¹æ¬¡ä¿¡æ¯:")
        print(f"  - batch_x: {batch_x.shape}")
        print(f"  - batch_y: {batch_y.shape}")
        print(f"  - æ ‡ç­¾å€¼: {batch_y[:8]}")  # æ˜¾ç¤ºå‰8ä¸ªæ ‡ç­¾
        break
        
except Exception as e:
    print(f"\nâœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•æ¨¡å‹ä¸æ•°æ®çš„å…¼å®¹æ€§
print("\n" + "="*50)
print("5. æµ‹è¯•æ¨¡å‹ä¸çœŸå®æ•°æ®çš„å…¼å®¹æ€§...")

if 'train_loader' in locals():
    try:
        # ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•å‰å‘ä¼ æ’­
        batch_x = batch_x.to(device)
        batch_x_mark = batch_x_mark.to(device)
        
        with torch.no_grad():
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            output = model(batch_x, batch_x_mark)
            
        print("âœ“ æ¨¡å‹æˆåŠŸå¤„ç†çœŸå®DEAPæ•°æ®!")
        print(f"  - è¾“å…¥å½¢çŠ¶: {batch_x.shape}")
        print(f"  - è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æµ‹è¯•æŸå¤±è®¡ç®—
        criterion = torch.nn.CrossEntropyLoss()
        batch_y = batch_y.long().to(device)
        loss = criterion(output, batch_y)
        print(f"  - æŸå¤±å€¼: {loss.item():.4f}")
        
    except Exception as e:
        print(f"âœ— å¤„ç†çœŸå®æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

# æ€»ç»“
print("\n" + "="*70)
print("æµ‹è¯•æ€»ç»“")
print("="*70)

print("\nå¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œä½ å¯ä»¥:")
print("1. åˆ›å»ºå®Œæ•´çš„è®­ç»ƒè„šæœ¬")
print("2. å¼€å§‹è®­ç»ƒæ¨¡å‹")
print("\nå»ºè®®çš„ä¸‹ä¸€æ­¥:")
print("- å¦‚æœé‡åˆ°å†…å­˜é—®é¢˜ï¼Œè¿›ä¸€æ­¥å‡å°‘ llm_layers åˆ° 1")
print("- å¯ä»¥å°è¯•å¢åŠ  batch_size åˆ° 8 æˆ– 16")
print("- ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®ï¼Œæ‰€æœ‰è¢«è¯•æ–‡ä»¶éƒ½å­˜åœ¨")

print("\nç¥å®éªŒé¡ºåˆ©! ğŸš€")
